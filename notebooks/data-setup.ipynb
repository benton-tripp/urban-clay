{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../model\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import rasterio.warp\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.merge import merge\n",
    "from rasterio.windows import from_bounds\n",
    "# import stackstac\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import pickle as pkl\n",
    "from datetime import datetime, date, timedelta\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from shapely.geometry import box, mapping, Polygon, Point\n",
    "import heapq\n",
    "import torch\n",
    "import yaml\n",
    "from box import Box\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from torchvision.transforms import v2\n",
    "from src.model import ClayMAEModule\n",
    "\n",
    "# Define your data directory and tile size\n",
    "tiles_data_dir = \"../data/tiles_gdb\"\n",
    "if not os.path.exists(tiles_data_dir):\n",
    "    os.mkdir(tiles_data_dir)\n",
    "\n",
    "tiles_geojson_path = \"../data/tiles.geojson\"\n",
    "tile_size = 600\n",
    "\n",
    "# TO BE REMOVED\n",
    "if os.path.exists(tiles_geojson_path):\n",
    "    os.remove(tiles_geojson_path)\n",
    "\n",
    "county_boundary = gpd.read_file(\"../data/county_boundary.shp\")\n",
    "# Reproject to EPSG:32617 (UTM Zone 17N), which is the CRS used by \n",
    "# Sentinel-2 images in this region\n",
    "if county_boundary.crs != 32617:\n",
    "    county_boundary = county_boundary.to_crs(\"EPSG:32617\")\n",
    "    county_boundary.to_file(\"../data/county_boundary.shp\")\n",
    "\n",
    "minx, miny, maxx, maxy = county_boundary.total_bounds\n",
    "\n",
    "# Adjust the bounds so that they align with the tile size\n",
    "# For minx and miny, floor to the nearest multiple of tile_size\n",
    "minx = math.floor(minx / tile_size) * tile_size\n",
    "miny = math.floor(miny / tile_size) * tile_size\n",
    "# For maxx and maxy, ceil to the nearest multiple of tile_size\n",
    "maxx = math.ceil(maxx / tile_size) * tile_size\n",
    "maxy = math.ceil(maxy / tile_size) * tile_size\n",
    "\n",
    "# Generate tiles (i.e., bounding boxes in the format of xmin, ymin, xmax, ymax) \n",
    "# to fill the county boundary. If it overlaps the edge, extend outside (rather \n",
    "# than coming up short within the boundary), to ensure full coverage\n",
    "# Get the bounds of the county boundary\n",
    "\n",
    "if not os.path.exists(tiles_geojson_path):\n",
    "    # Generate the grid of tiles\n",
    "    all_tiles = []\n",
    "    x_coords = np.arange(minx, maxx, tile_size)\n",
    "    y_coords = np.arange(miny, maxy, tile_size)\n",
    "\n",
    "    for x in x_coords:\n",
    "        for y in y_coords:\n",
    "            # Create a box for each tile\n",
    "            tile = box(x, y, x + tile_size, y + tile_size)\n",
    "            all_tiles.append(tile)\n",
    "    # Create a GeoDataFrame for the tiles\n",
    "    tiles_gdf = gpd.GeoDataFrame({'geometry': all_tiles}, crs=county_boundary.crs)\n",
    "    # Keep only tiles that intersect the county, but keep their full geometry\n",
    "    tiles_gdf = tiles_gdf[tiles_gdf.geometry.intersects(county_boundary.union_all())]\\\n",
    "        .reset_index(drop=True)\n",
    "    # Remove duplicate tiles\n",
    "    tiles_gdf = tiles_gdf.drop_duplicates()\n",
    "    # Add columns for tracking satellite data\n",
    "    tiles_gdf['processed'] = False\n",
    "    tiles_gdf['data_files'] = [[] for _ in range(len(tiles_gdf))]\n",
    "    # Save tiles_gdf to GeoJSON\n",
    "    tiles_gdf.to_file(tiles_geojson_path, driver='GeoJSON')\n",
    "else:\n",
    "    tiles_gdf = gpd.read_file(tiles_geojson_path)\n",
    "\n",
    "# Generate raster and shapefile masks that match the external \n",
    "# bounds (and crs) of all of the tiles since it will not perfectly align with \n",
    "# the original boundary\n",
    "\n",
    "\n",
    "# Create a unified boundary from all the tiles\n",
    "tiles_union = tiles_gdf.union_all()\n",
    "\n",
    "# Save the unified tiles boundary as a shapefile mask\n",
    "tiles_union_gdf = gpd.GeoDataFrame(geometry=[tiles_union], crs=tiles_gdf.crs)\n",
    "tiles_union_shapefile_path = \"../data/tiles_union_mask.shp\"\n",
    "tiles_union_gdf.to_file(tiles_union_shapefile_path)\n",
    "\n",
    "# Create a raster mask matching the external bounds and CRS of all tiles\n",
    "# Define the raster properties\n",
    "minx_tiles, miny_tiles, maxx_tiles, maxy_tiles = tiles_union.bounds\n",
    "pixel_size = 10  # Pixel size in meters (adjust as needed)\n",
    "\n",
    "# Calculate the number of rows and columns\n",
    "ncols = int((maxx_tiles - minx_tiles) / pixel_size)\n",
    "nrows = int((maxy_tiles - miny_tiles) / pixel_size)\n",
    "\n",
    "# Define the transform for the raster\n",
    "transform = rasterio.transform.from_origin(minx_tiles, maxy_tiles, \n",
    "                                           pixel_size, pixel_size)\n",
    "\n",
    "# Define the raster metadata\n",
    "raster_meta = {\n",
    "    'driver': 'GTiff',\n",
    "    'height': nrows,\n",
    "    'width': ncols,\n",
    "    'count': 1,\n",
    "    'dtype': 'uint8',\n",
    "    'crs': tiles_gdf.crs,\n",
    "    'transform': transform\n",
    "}\n",
    "\n",
    "# Path to save the raster mask\n",
    "raster_mask_path = \"../data/tiles_union_mask.tif\"\n",
    "\n",
    "# Create the raster mask\n",
    "with rasterio.open(raster_mask_path, 'w', **raster_meta) as out_raster:\n",
    "    # Burn the unified tiles geometry into the raster\n",
    "    shapes = [(tiles_union, 1)]\n",
    "    burned = rasterio.features.rasterize(\n",
    "        shapes,\n",
    "        out_shape=(nrows, ncols),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        all_touched=True,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    out_raster.write_band(1, burned)\n",
    "\n",
    "# Visualize the tiles and vectors (optional)\n",
    "ax = county_boundary.plot(color='lightgray', figsize=(7, 7))\n",
    "tiles_gdf.boundary.plot(ax=ax, edgecolor='red', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The data region is sub-divided into {len(tiles_gdf)} distinct tiles.\")\n",
    "\n",
    "# Calculate the width and height of each tile\n",
    "tiles_gdf['width'] = tiles_gdf.geometry.bounds.apply(lambda row: row.maxx - row.minx, axis=1)\n",
    "tiles_gdf['height'] = tiles_gdf.geometry.bounds.apply(lambda row: row.maxy - row.miny, axis=1)\n",
    "\n",
    "# Check if all tiles have the same width and height\n",
    "unique_widths = tiles_gdf['width'].unique()\n",
    "unique_heights = tiles_gdf['height'].unique()\n",
    "\n",
    "if len(unique_widths) == 1 and len(unique_heights) == 1:\n",
    "    print(f\"All tiles have uniform size: width = {unique_widths[0]} meters, height = {unique_heights[0]} meters.\")\n",
    "else:\n",
    "    print(\"Tiles have varying sizes:\")\n",
    "    print(tiles_gdf[['width', 'height']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban Imperviousness Data - https://www.sciencebase.gov/catalog/item/604a500ed34eb120311b006c\n",
    "\n",
    "# Path to urban imperviousness data\n",
    "ui2016_path = \"//tsclient/D/data/nlcd_2016_impervious_l48_20210604/nlcd_2016_impervious_l48_20210604.img\"\n",
    "\n",
    "urban_data_dir = \"../data/urban_gdb\"\n",
    "if not os.path.exists(urban_data_dir):\n",
    "    os.mkdir(urban_data_dir)\n",
    "\n",
    "def urban_file_name(year, data_dir):\n",
    "    return os.path.join(data_dir, f\"nlcd_impervious_{year}.tif\")\n",
    "\n",
    "def extract_urban_by_year_tile_by_tile(ui_path, year, tiles_gdf, urban_data_dir=\"../data/urban_gdb\", \n",
    "                                       target_resolution=10):\n",
    "    \"\"\"\n",
    "    Process and extract urban imperviousness data for each tile to avoid memory errors and resample to target resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - ui_path (str): Path to the urban imperviousness raster data.\n",
    "    - year (int): Year of the data (used for naming purposes).\n",
    "    - tiles_gdf (GeoDataFrame): GeoDataFrame containing the tiles.\n",
    "    - urban_data_dir (str): Directory to save the urban imperviousness tiles.\n",
    "    - target_resolution (int): Target resolution for resampling, in meters (default is 10m).\n",
    "\n",
    "    Returns:\n",
    "    - tiles_gdf (GeoDataFrame): Updated GeoDataFrame with paths to the urban imperviousness tiles.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(urban_data_dir):\n",
    "        os.makedirs(urban_data_dir)\n",
    "\n",
    "    # Reproject tiles_gdf to match the CRS of the urban imperviousness data\n",
    "    with rasterio.open(ui_path) as ui_dataset:\n",
    "        ui_crs = ui_dataset.crs\n",
    "    tiles_gdf_ui_crs = tiles_gdf.to_crs(ui_crs)\n",
    "\n",
    "    # Add a new column in tiles_gdf for the urban imperviousness tile path if it doesn't exist\n",
    "    if 'ui_tile_path' not in tiles_gdf.columns:\n",
    "        tiles_gdf['ui_tile_path'] = None\n",
    "\n",
    "    for idx, tile in tiles_gdf_ui_crs.iterrows():\n",
    "        # Check if the tile has already been processed\n",
    "        ui_tile_filename = f\"ui_tile_{year}_{idx}.tif\"\n",
    "        ui_tile_path = os.path.join(urban_data_dir, ui_tile_filename)\n",
    "        if os.path.exists(ui_tile_path):\n",
    "            # Update the tiles_gdf\n",
    "            tiles_gdf.at[idx, 'ui_tile_path'] = ui_tile_path\n",
    "            continue\n",
    "        \n",
    "        # Get the tile bounds\n",
    "        minx, miny, maxx, maxy = tile.geometry.bounds\n",
    "\n",
    "        # Open the dataset within the loop\n",
    "        with rasterio.open(ui_path) as ui_dataset:\n",
    "            # Define the window to read\n",
    "            window = from_bounds(minx, miny, maxx, maxy, transform=ui_dataset.transform)\n",
    "\n",
    "            # Read the data within the window\n",
    "            try:\n",
    "                ui_tile_data = ui_dataset.read(1, window=window)\n",
    "            except ValueError:\n",
    "                print(f\"No data for tile {idx}\")\n",
    "                continue\n",
    "\n",
    "            # Check if the tile contains data\n",
    "            if np.all(ui_tile_data == ui_dataset.nodata):\n",
    "                print(f\"No data in tile {idx}\")\n",
    "                continue\n",
    "\n",
    "            # Define the transform for the tile\n",
    "            tile_transform = ui_dataset.window_transform(window)\n",
    "\n",
    "            # Calculate the resampling scale factors for both dimensions\n",
    "            scale_x = ui_dataset.res[0] / target_resolution\n",
    "            scale_y = ui_dataset.res[1] / target_resolution\n",
    "\n",
    "            # Create a destination array for the resampled data\n",
    "            dest_shape = (\n",
    "                int(ui_tile_data.shape[0] * scale_y),\n",
    "                int(ui_tile_data.shape[1] * scale_x)\n",
    "            )\n",
    "            dest_array = np.empty(dest_shape, dtype=ui_tile_data.dtype)\n",
    "\n",
    "            # Reproject and resample to the target resolution\n",
    "            rasterio.warp.reproject(\n",
    "                source=ui_tile_data,\n",
    "                destination=dest_array,\n",
    "                src_transform=tile_transform,\n",
    "                src_crs=ui_dataset.crs,\n",
    "                dst_transform=rasterio.transform.from_origin(minx, maxy, target_resolution, target_resolution),\n",
    "                dst_crs=ui_dataset.crs,\n",
    "                resampling=Resampling.bilinear  # Choose the resampling method\n",
    "            )\n",
    "\n",
    "        # Save the resampled tile to a GeoTIFF\n",
    "        with rasterio.open(\n",
    "            ui_tile_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=dest_array.shape[0],\n",
    "            width=dest_array.shape[1],\n",
    "            count=1,\n",
    "            dtype=dest_array.dtype,\n",
    "            crs=ui_dataset.crs,\n",
    "            transform=rasterio.transform.from_origin(minx, maxy, target_resolution, target_resolution),\n",
    "            nodata=ui_dataset.nodata\n",
    "        ) as dst:\n",
    "            dst.write(dest_array, 1)\n",
    "\n",
    "        # Update the tiles_gdf\n",
    "        tiles_gdf.at[idx, 'ui_tile_path'] = ui_tile_path\n",
    "\n",
    "    # Reproject tiles_gdf back to its original CRS if necessary\n",
    "    tiles_gdf = tiles_gdf.to_crs(tiles_gdf.crs)\n",
    "\n",
    "    # Save the updated tiles_gdf\n",
    "    tiles_geojson_path = \"../data/tiles.geojson\"\n",
    "    tiles_gdf.to_file(tiles_geojson_path, driver='GeoJSON')\n",
    "\n",
    "    return tiles_gdf\n",
    "\n",
    "\n",
    "tiles_gdf = extract_urban_by_year_tile_by_tile(\n",
    "    ui_path=ui2016_path,\n",
    "    year=2016,\n",
    "    tiles_gdf=tiles_gdf,\n",
    "    urban_data_dir=urban_data_dir\n",
    ")\n",
    "# Save the updated tiles_gdf to 'tiles.geojson'\n",
    "tiles_gdf.to_file(tiles_geojson_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and variables\n",
    "mosaic_path = os.path.join(\"../data\", \"urban_imperviousness.tif\")\n",
    "\n",
    "# CRS to match (assuming both county_boundary and tiles_gdf have the same CRS)\n",
    "target_crs = county_boundary.crs\n",
    "\n",
    "if not os.path.exists(mosaic_path):\n",
    "    # Get the list of urban imperviousness tile paths\n",
    "    tile_paths = tiles_gdf['ui_tile_path'].dropna().tolist()\n",
    "\n",
    "    # List to hold rasterio datasets\n",
    "    src_files_to_mosaic = []\n",
    "\n",
    "    # Read each tile and append to the list\n",
    "    for fp in tile_paths:\n",
    "        src = rasterio.open(fp)\n",
    "        src_files_to_mosaic.append(src)\n",
    "\n",
    "    # Merge the tiles into a mosaic\n",
    "    mosaic_array, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "    # Close the individual datasets\n",
    "    for src in src_files_to_mosaic:\n",
    "        src.close()\n",
    "\n",
    "    # Get metadata from one of the source files (they should all be the same)\n",
    "    with rasterio.open(tile_paths[0]) as src0:\n",
    "        out_meta = src0.meta.copy()\n",
    "\n",
    "    # Update the metadata to reflect the mosaic dimensions and transform\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic_array.shape[1],\n",
    "        \"width\": mosaic_array.shape[2],\n",
    "        \"transform\": out_trans,\n",
    "        \"crs\": src0.crs  # This is the original CRS of the mosaic\n",
    "    })\n",
    "\n",
    "    # Reproject the mosaic array to the target CRS (county_boundary and tiles_gdf CRS)\n",
    "    dst_array, dst_transform = rasterio.warp.reproject(\n",
    "        source=mosaic_array,\n",
    "        src_crs=out_meta['crs'],\n",
    "        src_transform=out_trans,\n",
    "        dst_crs=target_crs,  # Target CRS from county_boundary and tiles_gdf\n",
    "        dst_transform=None,  # Let rasterio calculate the new transform\n",
    "        resampling=rasterio.enums.Resampling.bilinear  # Choose appropriate resampling method\n",
    "    )\n",
    "\n",
    "    # Update metadata to reflect the new dimensions and CRS after reprojection\n",
    "    out_meta.update({\n",
    "        \"height\": dst_array.shape[1],\n",
    "        \"width\": dst_array.shape[2],\n",
    "        \"transform\": dst_transform,\n",
    "        \"crs\": target_crs\n",
    "    })\n",
    "\n",
    "    # Save the reprojected mosaic to a file\n",
    "    with rasterio.open(mosaic_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(dst_array)\n",
    "    \n",
    "    # Open the saved reprojected mosaic\n",
    "    mosaic = rasterio.open(mosaic_path)\n",
    "else:\n",
    "    # Open the mosaic from file\n",
    "    mosaic = rasterio.open(mosaic_path)\n",
    "\n",
    "# Read the data from the mosaic\n",
    "data = mosaic.read(1)  # Read the first band\n",
    "\n",
    "# Plot the mosaic\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Use mosaic.transform for the transform parameter\n",
    "rasterio.plot.show(data, transform=mosaic.transform, ax=ax, cmap='gray_r')\n",
    "\n",
    "# Plot the county boundary and tile boundaries\n",
    "county_boundary.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=1)\n",
    "tiles_gdf.boundary.plot(ax=ax, edgecolor='blue', linewidth=0.5)\n",
    "\n",
    "plt.title('Urban Imperviousness 2016 Tile Grid')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_small_files(directory, min_size_kb):\n",
    "    \"\"\"\n",
    "    Remove all files from the directory that are smaller than min_size_kb kilobytes.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The path to the directory.\n",
    "    min_size_kb (int or float): The minimum size of the file in kilobytes. Files smaller than this will be removed.\n",
    "    \"\"\"\n",
    "    # Convert kilobytes to bytes\n",
    "    min_size_bytes = min_size_kb * 1024\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if it's a file\n",
    "        if os.path.isfile(file_path):\n",
    "            # Get the file size in bytes\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            \n",
    "            # Remove the file if it's smaller than the threshold\n",
    "            if file_size < min_size_bytes:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed: {filename}, Size: {file_size / 1024:.2f} KB\")\n",
    "\n",
    "def remove_newest_files(directory, n):\n",
    "    \"\"\"\n",
    "    Remove up to n of the newest files from the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The path to the directory.\n",
    "    n (int): The number of newest files to remove.\n",
    "    \"\"\"\n",
    "    # Get a list of all files in the directory with their modification times\n",
    "    files_with_times = [\n",
    "        (os.path.join(directory, filename), os.path.getmtime(os.path.join(directory, filename)))\n",
    "        for filename in os.listdir(directory)\n",
    "        if os.path.isfile(os.path.join(directory, filename))\n",
    "    ]\n",
    "    \n",
    "    # Sort the files by their modification time (newest first)\n",
    "    newest_files = heapq.nlargest(n, files_with_times, key=lambda x: x[1])\n",
    "\n",
    "    # Remove up to n of the newest files\n",
    "    for file_path, mod_time in newest_files:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed: {os.path.basename(file_path)}, Modified on: {datetime.fromtimestamp(mod_time)}\")\n",
    "\n",
    "# Clean up files where they did not download all the way, or where the connection may have been interrupted.\n",
    "remove_small_files(tiles_data_dir, 1)\n",
    "# remove_newest_files(tiles_data_dir, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_available_dates(tile_geometry, catalog, start, end, query, collections=[\"sentinel-2-l2a\"], limit=1000):\n",
    "    # Get the bounds of the tile in WGS84\n",
    "    tile_wgs84 = gpd.GeoSeries([tile_geometry], crs=\"EPSG:32617\").to_crs(\"EPSG:4326\").iloc[0]\n",
    "    minx, miny, maxx, maxy = tile_wgs84.bounds\n",
    "    bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Perform the search\n",
    "    search = catalog.search(\n",
    "        collections=collections,\n",
    "        bbox=bbox,\n",
    "        datetime=f\"{start}/{end}\",\n",
    "        limit=limit,\n",
    "        query=query\n",
    "    )\n",
    "\n",
    "    # Get the items from the search results and collect dates\n",
    "    items = list(search.items())\n",
    "    available_dates = [item.datetime.date() for item in items]\n",
    "    return available_dates\n",
    "\n",
    "\n",
    "def select_dates_best_spread(dates_list, num_per_year=4):\n",
    "    # Sort the dates list\n",
    "    dates_list.sort()\n",
    "    \n",
    "    # Determine target months based on the desired number per year\n",
    "    target_months = {\n",
    "        4: [3, 6, 9, 12],  # Default quarters: March, June, September, December\n",
    "        3: [4, 8, 12],     # For 3 dates per year: April, August, December\n",
    "        2: [6, 12],        # For 2 dates per year: June, December\n",
    "        1: [6]             # For 1 date per year: June\n",
    "    }.get(num_per_year, [6])  # Default to June if an unexpected `num_per_year` is given\n",
    "    \n",
    "    # Group dates by year\n",
    "    dates_by_year = defaultdict(list)\n",
    "    for d in dates_list:\n",
    "        dates_by_year[d.year].append(d)\n",
    "    \n",
    "    selected_dates = []\n",
    "    \n",
    "    # Iterate over each year and select the best spread dates\n",
    "    for year in sorted(dates_by_year.keys()):\n",
    "        available_dates = dates_by_year[year]\n",
    "        \n",
    "        for month in target_months:\n",
    "            # Set the target date as the 1st of the target month\n",
    "            target_date_in_year = date(year, month, 1)\n",
    "            \n",
    "            # Find the closest date in the available dates to the target date\n",
    "            closest_date = min(available_dates, key=lambda d: abs(d - target_date_in_year))\n",
    "            \n",
    "            # Check if the selected date is not already in the list\n",
    "            if closest_date not in selected_dates:\n",
    "                selected_dates.append(closest_date)\n",
    "    \n",
    "    # Post-process to ensure cross-year spread is handled\n",
    "    selected_dates.sort()\n",
    "    return selected_dates\n",
    "\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1/\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "query = {\"eo:cloud_cover\": {\"lt\": 1}}\n",
    "\n",
    "collections=[\"sentinel-2-l2a\"]\n",
    "\n",
    "# Dates to query\n",
    "start = \"2016-01-01\"\n",
    "end = \"2024-08-31\"\n",
    "\n",
    "if not os.path.exists(\"../data/available_dates.pkl\"):\n",
    "    available_dates = query_available_dates(tiles_gdf.iloc[0].geometry, catalog, start, end, query)\n",
    "    with open(\"../data/available_dates.pkl\", \"wb\") as f:\n",
    "        pkl.dump(available_dates, f)\n",
    "else:\n",
    "    with open(\"../data/available_dates.pkl\", \"rb\") as f:\n",
    "        available_dates = pkl.load(f)\n",
    "\n",
    "print(\"Number of available dates for processing:\", len(available_dates))\n",
    "\n",
    "selected_dates = select_dates_best_spread([d for d in available_dates if d !=  date(2017, 5, 16) \n",
    "                                           and d != date(2024, 8, 27)], 3) # 5/16 is basically unavailable, and 8/25 is already there\n",
    "\n",
    "print(\"Number of selected dates for processing:\", len(selected_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bands to query:\n",
    "#  B02 (Blue) - 490 nm (10 m resolution)\n",
    "#      Useful for water body delineation and identifying vegetation.\n",
    "#  B03 (Green) - 560 nm (10 m resolution)\n",
    "#      Useful for vegetation monitoring.\n",
    "#  B04 (Red) - 665 nm (10 m resolution)\n",
    "#      Useful for vegetation and crop health monitoring (e.g., chlorophyll absorption).\n",
    "#  B08 (Near Infrared - NIR) - 842 nm (10 m resolution)\n",
    "#      Key band for calculating NDVI and monitoring vegetation health.\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\"]\n",
    "bands_map = {\"B02\": \"blue\", \"B03\": \"green\", \"B04\": \"red\", \"B08\": \"nir\"}\n",
    "\n",
    "# GSD (in meters)\n",
    "gsd = 10\n",
    "\n",
    "epsg = 32617\n",
    "\n",
    "# Function to query STAC items for a tile\n",
    "def query_stac_tile(tile_geometry, catalog, start, end, \n",
    "                    query, collections=[\"sentinel-2-l2a\"], \n",
    "                    limit=1000):\n",
    "    # Get the bounds of the tile in WGS84\n",
    "    tile_wgs84 = gpd.GeoSeries([tile_geometry], crs=\"EPSG:32617\").to_crs(\"EPSG:4326\").iloc[0]\n",
    "    minx, miny, maxx, maxy = tile_wgs84.bounds\n",
    "    bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Perform the search\n",
    "    search = catalog.search(\n",
    "        collections=collections,\n",
    "        bbox=bbox,\n",
    "        datetime=f\"{start}/{end}\",\n",
    "        limit=limit,\n",
    "        query=query\n",
    "    )\n",
    "    # Get the items from the search results\n",
    "    items = list(search.item_collection())\n",
    "    return items\n",
    "\n",
    "def get_subregion(dataset, bounds):\n",
    "    min_x, min_y, max_x, max_y = bounds\n",
    "    # Subset the dataset using xarray's sel function\n",
    "    subregion = dataset.sel(\n",
    "        x=slice(min_x, max_x),  # X-coordinate bounds\n",
    "        y=slice(max_y, min_y)   # Y-coordinate bounds (flip due to coordinate system)\n",
    "    )\n",
    "    return subregion\n",
    "\n",
    "# Function to get images from item\n",
    "def get_images_from_item(item, bands, output_file_path, \n",
    "                         chunk_size=2048, dtype=np.float32, \n",
    "                         crs=\"EPSG:32617\", bounds=None):\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "\n",
    "        band_datasets = []\n",
    "        # Loop through bands and collect data\n",
    "        for band in bands:\n",
    "            # Sign the asset URLs\n",
    "            asset_href = planetary_computer.sign(item.assets[band].href)\n",
    "            # Open the image using rioxarray\n",
    "            with rasterio.Env():\n",
    "                ds = rioxarray.open_rasterio(\n",
    "                    asset_href,\n",
    "                    chunks={\"band\": -1, \"x\": chunk_size, \"y\": chunk_size},\n",
    "                    lock=False\n",
    "                ).astype(dtype)\n",
    "                if ds.rio.crs != crs:\n",
    "                    ds = ds.rio.reproject(\n",
    "                        crs, \n",
    "                        resampling=Resampling.nearest,\n",
    "                        num_threads=2\n",
    "                    )\n",
    "                if bounds is not None:\n",
    "                    ds = get_subregion(ds, bounds)\n",
    "                band_datasets.append(ds)\n",
    "\n",
    "        # Stack bands into a single dataset\n",
    "        stacked_ds = xr.concat(band_datasets, dim='band')\n",
    "\n",
    "        # Store time as an attribute\n",
    "        naive_datetime = item.datetime.replace(tzinfo=None)\n",
    "        time_value = np.datetime64(naive_datetime, 'ns')\n",
    "        stacked_ds.attrs['time'] = str(time_value)\n",
    "        \n",
    "        # Save the stacked dataset to a single GeoTIFF file\n",
    "        # stacked_ds.rio.to_raster(output_file_path)\n",
    "        # Write the data using Dask and rioxarray, with windowed=True and tiled=True\n",
    "        with rasterio.Env(GDAL_CACHEMAX=512):  # Set cache size to 512 MB\n",
    "            stacked_ds.rio.to_raster(\n",
    "                output_file_path,\n",
    "                tiled=True,\n",
    "                windowed=True,\n",
    "                blockxsize=256,\n",
    "                blockysize=256,\n",
    "                compress=\"deflate\",\n",
    "                num_threads=2,\n",
    "                bigtiff='yes'\n",
    "            )\n",
    "    return output_file_path\n",
    "\n",
    "# Function to clean up bounds for filename\n",
    "def clean_bounds(bounds):\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    # Round to 3 decimal places and remove any special characters\n",
    "    minx_str = f\"{minx:.3f}\".replace('.', '_')\n",
    "    miny_str = f\"{miny:.3f}\".replace('.', '_')\n",
    "    maxx_str = f\"{maxx:.3f}\".replace('.', '_')\n",
    "    maxy_str = f\"{maxy:.3f}\".replace('.', '_')\n",
    "    # Combine into a single string\n",
    "    bounds_str = f\"{minx_str}_{miny_str}_{maxx_str}_{maxy_str}\"\n",
    "    return bounds_str\n",
    "\n",
    "# Parameters\n",
    "initial_buffer_days = 0\n",
    "max_buffer_days = 60  # Maximum buffer to apply\n",
    "tile_error = []\n",
    "failed_downloads = []\n",
    "not_found_items = []\n",
    "\n",
    "for idx, tile in tiles_gdf[~tiles_gdf['processed']].iterrows():\n",
    "    try:\n",
    "        # Clear previous output to keep the notebook clean\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Processing tile {idx + 1}/{len(tiles_gdf)}\")\n",
    "        \n",
    "        tile_geometry = tile['geometry']\n",
    "        \n",
    "        # Initialize list to store data file paths for the current tile\n",
    "        tile_data_files = tiles_gdf.at[idx, 'data_files'] if 'data_files' in tiles_gdf.columns else []\n",
    "\n",
    "        # Create a single progress bar for the entire tile processing\n",
    "        with tqdm(total=len(selected_dates), desc=f\"Tile {idx}: Processing dates\", unit=\" date\", leave=False) as pbar:\n",
    "            for query_date in selected_dates:\n",
    "                buffer_days = initial_buffer_days\n",
    "                found_item = False  # Flag to indicate if an item was found\n",
    "                \n",
    "                while not found_item and buffer_days <= max_buffer_days:\n",
    "                    start = query_date - timedelta(days=buffer_days)\n",
    "                    end = query_date + timedelta(days=buffer_days)\n",
    "                    \n",
    "                    # Define the output file path before querying\n",
    "                    tile_bounds = tile_geometry.bounds\n",
    "                    bounds_str = clean_bounds(tile_bounds)\n",
    "                    band_sfx = \"_\".join(bands)\n",
    "                    output_file = os.path.join(tiles_data_dir, f\"tile_{idx}_{bounds_str}_{query_date.isoformat()}_{band_sfx}.tif\")\n",
    "\n",
    "                    # Check if the file already exists\n",
    "                    if os.path.exists(output_file):\n",
    "                        tile_data_files.append(output_file)\n",
    "                        status_message = f\"\\nData file {output_file} already exists, skipping download\"\n",
    "                        pbar.set_postfix_str(status_message, refresh=False)\n",
    "                        pbar.update(1)\n",
    "                        found_item = True  # Skip to the next date since the file exists\n",
    "                        break\n",
    "                    \n",
    "                    # Query STAC items for the tile on this specific date with buffer\n",
    "                    tile_items = query_stac_tile(tile_geometry, catalog, start.isoformat(), end.isoformat(), query, collections=collections)\n",
    "                    \n",
    "                    if tile_items:\n",
    "                        # Collect unique items per date\n",
    "                        unique_items = {item.datetime.date().isoformat(): item for item in tile_items}\n",
    "                        \n",
    "                        for date_str, item in unique_items.items():\n",
    "                            # Update the output file path for the specific date found\n",
    "                            output_file = os.path.join(tiles_data_dir, f\"tile_{idx}_{bounds_str}_{date_str}_{band_sfx}.tif\")\n",
    "                            \n",
    "                            if not os.path.exists(output_file):\n",
    "                                try:\n",
    "                                    # Download and save the data\n",
    "                                    get_images_from_item(item, bands, output_file, chunk_size=2048, bounds=tile_bounds)\n",
    "                                    tile_data_files.append(output_file)\n",
    "                                    status_message = f\"\\nSaved data to {output_file}\"\n",
    "                                except Exception as e:\n",
    "                                    status_message = f\"\\nError processing item {item.id}: {e}\"\n",
    "                                    failed_downloads.append({\n",
    "                                        \"idx\": idx,\n",
    "                                        \"query_date\": query_date,\n",
    "                                        \"tile_geometry\": tile_geometry,\n",
    "                                        \"tile_bounds\": tile_bounds,\n",
    "                                        \"bounds_str\": bounds_str,\n",
    "                                        \"band_sfx\": band_sfx,\n",
    "                                        \"output_file\": output_file,\n",
    "                                        \"item\":item, \n",
    "                                        \"bands\":bands\n",
    "                                    })\n",
    "                                    continue  # Skip this item and move on to the next\n",
    "                            else:\n",
    "                                tile_data_files.append(output_file)\n",
    "                                status_message = f\"\\nData file {output_file} already exists, skipping download\"\n",
    "                            \n",
    "                            # Set the status message without duplicating the progress bar output\n",
    "                            pbar.set_postfix_str(status_message, refresh=False)\n",
    "                            pbar.update(1)\n",
    "                        \n",
    "                        found_item = True  # Mark as found to exit the while loop\n",
    "                    else:\n",
    "                        # Increment the buffer and try again\n",
    "                        buffer_days += 2\n",
    "                        tqdm.write(f\"\\nNo items found for tile {idx} on date {query_date} with buffer of {buffer_days} days\")\n",
    "\n",
    "                if not found_item:\n",
    "                    # No items found even with the maximum buffer\n",
    "                    tqdm.write(f\"\\nNo items found for tile {idx} on date {query_date} within the maximum buffer of {max_buffer_days} days\")\n",
    "                    not_found_items.append({\n",
    "                        \"idx\":idx,\n",
    "                        \"query_date\": query_date,\n",
    "                        \"tile_geometry\": tile_geometry,\n",
    "                        \"tile_bounds\": tile_bounds,\n",
    "                        \"bounds_str\": bounds_str,\n",
    "                        \"band_sfx\": band_sfx,\n",
    "                        \"output_file\": output_file\n",
    "                    })\n",
    "                    # Update the progress bar even if no items found\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # Update the GeoDataFrame with the accumulated data files\n",
    "        tiles_gdf.at[idx, 'data_files'] = tile_data_files\n",
    "        tiles_gdf.at[idx, 'processed'] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        tile_error.append(tile)\n",
    "        # Use tqdm.write to integrate with progress bar output\n",
    "        tqdm.write(f\"\\nError processing tile {idx}: {e}\")\n",
    "        continue  # Skip this tile and move on to the next one\n",
    "\n",
    "    # Save the updated tiles_gdf to 'tiles.geojson' after each tile\n",
    "    tiles_gdf.to_file(tiles_geojson_path, driver='GeoJSON')\n",
    "    \n",
    "with open('../data/tile_error.pkl', 'wb') as f:\n",
    "    pkl.dump(tile_error, f)\n",
    "\n",
    "with open('../data/failed_downloads.pkl', 'wb') as f:\n",
    "    pkl.dump(failed_downloads, f)\n",
    " \n",
    "with open('../data/not_found_items.pkl', 'wb') as f:\n",
    "    pkl.dump(not_found_items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes it quits out, make sure the gdf updated properly\n",
    "\n",
    "for idx, tile in tiles_gdf.iterrows():\n",
    "    tile_geometry = tile['geometry']\n",
    "    tile_bounds = tile_geometry.bounds\n",
    "    bounds_str = clean_bounds(tile_bounds)\n",
    "\n",
    "    # Initialize or retrieve the list to store data file paths for the current tile\n",
    "    tile_data_files = tiles_gdf.at[idx, 'data_files'] if 'data_files' in tiles_gdf.columns else []\n",
    "\n",
    "    for query_date in selected_dates:\n",
    "        output_file = os.path.join(\n",
    "            tiles_data_dir,\n",
    "            f\"tile_{idx}_{bounds_str}_{query_date.isoformat()}_{band_sfx}.tif\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            tile_data_files.append(output_file)\n",
    "        else:\n",
    "            # File does not exist; no action needed other than marking as processed\n",
    "            pass\n",
    "\n",
    "    # Update the GeoDataFrame\n",
    "    tiles_gdf.at[idx, 'data_files'] = tile_data_files\n",
    "    tiles_gdf.at[idx, 'processed'] = True\n",
    "\n",
    "# Save the updated GeoDataFrame to the GeoJSON file\n",
    "tiles_gdf.to_file(tiles_geojson_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse the date from the file name.\n",
    "    The date is assumed to be in ISO format (YYYY-MM-DD) within the file name.\n",
    "    The date is typically the part just before the band names, which we assume is the third-to-last part.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize the file path to handle potential backward slashes and extract the filename\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Split by underscore and extract the date part (which is third-to-last based on the example)\n",
    "        parts = base_name.split('_')\n",
    "        # The date part is the one right before the bands (3rd from last)\n",
    "        date_str = parts[-5]\n",
    "        # Convert the date string to a date object\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "    except (ValueError, IndexError) as e:\n",
    "        # In case of any error in parsing, return None and print error for debugging\n",
    "        print(f\"Error parsing date from filename '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "def assign_files_to_dates(tiles_gdf, max_buffer_days, selected_dates):\n",
    "    \"\"\"\n",
    "    Assign satellite image files to dates within a buffer period for each tile.\n",
    "\n",
    "    Parameters:\n",
    "    - tiles_gdf (GeoDataFrame): The GeoDataFrame containing tile geometries and file lists.\n",
    "    - max_buffer_days (int): The maximum buffer in days to assign files to a date.\n",
    "    - selected_dates (list of datetime.date): The selected target dates for which to assign files.\n",
    "\n",
    "    Returns:\n",
    "    - tiles_gdf (GeoDataFrame): The updated GeoDataFrame with a 'date_files_mapping' column,\n",
    "                                mapping each selected date to its associated files (stored as strings).\n",
    "    \"\"\"\n",
    "    # Initialize a new column in tiles_gdf to hold the date-to-files mapping\n",
    "    if 'date_files_mapping' not in tiles_gdf.columns:\n",
    "        tiles_gdf['date_files_mapping'] = [{} for _ in range(len(tiles_gdf))]\n",
    "\n",
    "    # Convert selected_dates to strings for GeoJSON compatibility\n",
    "    selected_dates_str = [d.strftime('%Y-%m-%d') for d in selected_dates]\n",
    "\n",
    "    for idx, tile in tiles_gdf.iterrows():\n",
    "        # Get the list of files associated with the current tile\n",
    "        data_files = tile['data_files']\n",
    "        \n",
    "        # Initialize a dictionary with each selected date (as string) as a key and an empty list as the value\n",
    "        date_files = {selected_date: [] for selected_date in selected_dates_str}\n",
    "        \n",
    "        for file in data_files:\n",
    "            # Extract the date from the file name\n",
    "            file_date = parse_date_from_filename(file)\n",
    "            if not file_date:\n",
    "                continue  # Skip files without valid dates\n",
    "\n",
    "            # Convert file_date to string for comparison\n",
    "            file_date_str = file_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Check if the file date is within the buffer for any of the selected dates\n",
    "            for target_date in selected_dates:\n",
    "                if abs((file_date - target_date).days) <= max_buffer_days:\n",
    "                    # Add the file to the list of files for the matching target date\n",
    "                    date_files[target_date.strftime('%Y-%m-%d')].append(file)\n",
    "        \n",
    "        # Update the 'date_files_mapping' column with the date-to-files dictionary\n",
    "        tiles_gdf.at[idx, 'date_files_mapping'] = date_files\n",
    "    \n",
    "    # Drop the old 'data_files' column since it's now replaced by 'date_files_mapping'\n",
    "    tiles_gdf = tiles_gdf.drop(columns=['data_files'])\n",
    "    \n",
    "    return tiles_gdf\n",
    "\n",
    "# Call the function to update the GeoDataFrame\n",
    "updated_tiles_gdf = assign_files_to_dates(tiles_gdf, max_buffer_days=45, selected_dates=selected_dates)\n",
    "\n",
    "# Save the updated GeoDataFrame to GeoJSON\n",
    "updated_tiles_geojson_path = \"../data/tiles_with_dates.geojson\"\n",
    "updated_tiles_gdf.to_file(updated_tiles_geojson_path, driver='GeoJSON')\n",
    "\n",
    "print(f\"Updated tiles GeoDataFrame saved to {updated_tiles_geojson_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(updated_tiles_geojson_path, 'r') as file:\n",
    "    geojson_data = json.load(file)\n",
    "\n",
    "# Initialize containers for results\n",
    "empty_date_records = []\n",
    "multiple_file_records = []\n",
    "non_empty_counts = defaultdict(int)\n",
    "\n",
    "# Iterate over the features\n",
    "for feature in geojson_data['features']:\n",
    "    date_files_mapping = feature['properties']['date_files_mapping']\n",
    "    \n",
    "    # Check for empty lists and lists with more than one file\n",
    "    for date, file_list in date_files_mapping.items():\n",
    "        if len(file_list) == 0:\n",
    "            empty_date_records.append((feature['geometry']['coordinates'], date))\n",
    "        if len(file_list) > 1:\n",
    "            multiple_file_records.append((feature['geometry']['coordinates'], date, file_list))\n",
    "        \n",
    "        # Count non-empty lists per date\n",
    "        if len(file_list) > 0:\n",
    "            non_empty_counts[date] += 1\n",
    "            \n",
    "# Records with empty file lists\n",
    "if empty_date_records:\n",
    "    print(\"Records with empty file lists:\")\n",
    "    for record in empty_date_records:\n",
    "        print(f\"Coordinates: {record[0]}, Date: {record[1]}\")\n",
    "else:\n",
    "    print(\"No records with empty file lists found.\")\n",
    "\n",
    "# Records with more than one file in their list\n",
    "if multiple_file_records:\n",
    "    print(\"\\nRecords with more than one file:\")\n",
    "    for record in multiple_file_records:\n",
    "        print(f\"Coordinates: {record[0]}, Date: {record[1]}, Files: {record[2]}\")\n",
    "else:\n",
    "    print(\"No records with more than one file found.\")\n",
    "\n",
    "# Non-empty list counts per date\n",
    "print(\"\\nNon-empty file list counts per date:\")\n",
    "for date, count in non_empty_counts.items():\n",
    "    print(f\"Date: {date}, Count of non-empty lists: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tile coordinates and flatten them to ensure they are tuples of floats\n",
    "missing_tiles_coords = [tuple([tuple(coord) for coord in record[0]]) for record in empty_date_records]\n",
    "missing_tiles_coords = [i[0] for i in missing_tiles_coords]\n",
    "\n",
    "# Convert coordinates to polygons for plotting\n",
    "missing_tiles_gdf = gpd.GeoDataFrame(\n",
    "    geometry=[Polygon(coords) for coords in missing_tiles_coords],  # Ensure valid polygons\n",
    "    crs=updated_tiles_gdf.crs\n",
    ").drop_duplicates()\n",
    "\n",
    "# Visualize the tiles, filling in each tile with ANY missing dates (using the unique coordinates from empty_date_records)\n",
    "ax = county_boundary.plot(color='lightgray', figsize=(7, 7))\n",
    "updated_tiles_gdf.boundary.plot(ax=ax, edgecolor='red', linewidth=0.5)\n",
    "# Fill empty tiles with black\n",
    "missing_tiles_gdf.plot(ax=ax, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requery_missing_tiles(missing_tiles_coords, selected_dates, catalog, query, \n",
    "                          collections=[\"sentinel-2-l2a\"], limit=1000, \n",
    "                          save_dir=\"../data/missing_tiles\"):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for tile_coord in missing_tiles_coords:\n",
    "        tile_polygon = Polygon(tile_coord)  # Extract coordinates and create a Polygon\n",
    "        for query_date in selected_dates:\n",
    "            buffer_days = 0  # Start without buffer\n",
    "            \n",
    "            print(f\"Re-querying tile for date: {query_date} at coordinates: {tile_coord}\")\n",
    "            \n",
    "            while buffer_days <= 45:  # Maximum buffer is 45 days\n",
    "                start_date = query_date - timedelta(days=buffer_days)\n",
    "                end_date = query_date + timedelta(days=buffer_days)\n",
    "                \n",
    "                # Query the STAC catalog for the missing tile\n",
    "                items = query_stac_tile(tile_polygon, catalog, start_date.isoformat(), end_date.isoformat(), query, collections=collections, limit=limit)\n",
    "                \n",
    "                if items:\n",
    "                    for item in items:\n",
    "                        # Define the output file path based on tile and date\n",
    "                        output_file = os.path.join(save_dir, f\"tile_{query_date}_{buffer_days}_requery.tif\")\n",
    "                        \n",
    "                        # Save the queried image\n",
    "                        get_images_from_item(item, bands, output_file, bounds=tile_polygon.bounds)\n",
    "                        print(f\"Saved re-queried tile to {output_file}\")\n",
    "                    \n",
    "                    break  # Stop if data is found and saved\n",
    "                \n",
    "                buffer_days += 2  # Increase the buffer by 2 days if no data found\n",
    "\n",
    "            if buffer_days > 45:\n",
    "                print(f\"Failed to retrieve data for tile {tile_coord} on date {query_date} within buffer range.\")\n",
    "\n",
    "requery_missing_tiles(missing_tiles_coords, selected_dates, catalog, query=query, save_dir=\"../data/missing_tiles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_by_idx(tile_idx, tiles_gdf, bands):\n",
    "    tile_data_files = tiles_gdf.at[tile_idx, 'data_files']\n",
    "    datasets = []\n",
    "    for data_file in tile_data_files:\n",
    "        # print(data_file)\n",
    "        ds = rioxarray.open_rasterio(data_file)\n",
    "        date_str = ds.time\n",
    "        date = np.datetime64(date_str, 'ns')\n",
    "        ds = ds.assign_coords(band=bands)\n",
    "        ds = ds.expand_dims({'time': [date]})\n",
    "        datasets.append(ds)\n",
    "    stacked_ds = xr.concat(datasets, dim='time')\n",
    "    return stacked_ds\n",
    "\n",
    "stack = get_by_idx(0, tiles_gdf, bands)\n",
    "\n",
    "stack.sel(band=[\"B02\", \"B03\", \"B04\"]).plot.imshow(\n",
    "    row=\"time\", rgb=\"band\", vmin=0, vmax=2000, col_wrap=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mosaics_by_indices(indices, tiles_gdf, bands, start_date, end_date):\n",
    "    # Convert start and end dates to numpy datetime64\n",
    "    start_date_np = np.datetime64(start_date)\n",
    "    end_date_np = np.datetime64(end_date)\n",
    "\n",
    "    # Collect all data files for the specified indices\n",
    "    data_files = []\n",
    "    for idx in indices:\n",
    "        tile_data_files = tiles_gdf.at[idx, 'data_files']\n",
    "        for data_file in tile_data_files:\n",
    "            with rioxarray.open_rasterio(data_file) as ds:\n",
    "                date_str = ds.time\n",
    "            if date_str:\n",
    "                date = np.datetime64(date_str, 'ns')\n",
    "            else:\n",
    "                continue  # Skip if date is unavailable\n",
    "            # Filter by date range\n",
    "            if start_date_np <= date <= end_date_np:\n",
    "                data_files.append({'idx': idx, 'data_file': data_file, 'date': date})\n",
    "\n",
    "    # Get unique dates within the date range\n",
    "    dates = sorted(set(f['date'] for f in data_files))\n",
    "\n",
    "    mosaics = []\n",
    "\n",
    "    for date in dates:\n",
    "        # Collect data files for this date\n",
    "        files_for_date = [f for f in data_files if f['date'] == date]\n",
    "\n",
    "        # Open rasterio datasets for this date\n",
    "        rasterio_datasets = []\n",
    "        for f in files_for_date:\n",
    "            src = rasterio.open(f['data_file'])\n",
    "            rasterio_datasets.append(src)\n",
    "\n",
    "        if not rasterio_datasets:\n",
    "            continue  # Skip if no datasets are available for this date\n",
    "\n",
    "        # Merge the datasets into a mosaic\n",
    "        mosaic_array, out_trans = merge(rasterio_datasets)\n",
    "\n",
    "        # Get metadata from one of the datasets\n",
    "        out_meta = rasterio_datasets[0].meta.copy()\n",
    "        out_meta.update({\n",
    "            \"height\": mosaic_array.shape[1],\n",
    "            \"width\": mosaic_array.shape[2],\n",
    "            \"transform\": out_trans\n",
    "        })\n",
    "\n",
    "        # Generate coordinate arrays\n",
    "        transform = out_trans\n",
    "        height = out_meta['height']\n",
    "        width = out_meta['width']\n",
    "        res_x = transform.a  # Pixel width\n",
    "        res_y = -transform.e  # Pixel height (negative due to coordinate system)\n",
    "\n",
    "        x_coords = np.arange(width) * res_x + transform.c + res_x / 2\n",
    "        y_coords = np.arange(height) * res_y + transform.f + res_y / 2\n",
    "\n",
    "        # Create xarray DataArray from the mosaic\n",
    "        mosaic_da = xr.DataArray(\n",
    "            mosaic_array,\n",
    "            dims=('band', 'y', 'x'),\n",
    "            coords={\n",
    "                'band': bands,\n",
    "                'y': y_coords,\n",
    "                'x': x_coords\n",
    "            },\n",
    "            attrs={\n",
    "                'transform': transform,\n",
    "                'crs': out_meta['crs']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add time coordinate\n",
    "        mosaic_da = mosaic_da.expand_dims({'time': [date]})\n",
    "\n",
    "        mosaics.append(mosaic_da)\n",
    "\n",
    "        # Close datasets to free resources\n",
    "        for src in rasterio_datasets:\n",
    "            src.close()\n",
    "\n",
    "    if not mosaics:\n",
    "        print(\"No data available for the specified indices and date range.\")\n",
    "        return None\n",
    "\n",
    "    # Stack the mosaics over time\n",
    "    stacked_mosaics = xr.concat(mosaics, dim='time')\n",
    "\n",
    "    return stacked_mosaics\n",
    "\n",
    "\n",
    "# Define your parameters\n",
    "indices = [0, 1, 2, 3, 4, 5, 6, 7]  # List of tile indices\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-08-31'\n",
    "bands = ['B02', 'B03', 'B04', 'B08']\n",
    "\n",
    "# Call the function\n",
    "stacked_mosaics = get_mosaics_by_indices(indices, tiles_gdf, bands, start_date, end_date)\n",
    "\n",
    "# Plot the mosaics (e.g., the RGB composite for the first time step)\n",
    "if stacked_mosaics is not None:\n",
    "    stacked_mosaics.sel(band=['B04', 'B03', 'B02']).plot.imshow(\n",
    "        row='time', rgb='band', vmin=0, vmax=3000, col_wrap=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the urban data and satellite data directories\n",
    "urban_data_folder = \"../data/urban_gdb\"\n",
    "satellite_data_folder = \"../data/tiles_gdb\"\n",
    "\n",
    "# Specific date for the satellite data\n",
    "specific_date = \"2016-01-02\"\n",
    "\n",
    "# Only load files for tile_0 in the urban dataset\n",
    "urban_tile_files = glob.glob(os.path.join(urban_data_folder, \"tile_0_*.tif\"))\n",
    "\n",
    "# List to hold the opened rasterio datasets for the urban data\n",
    "urban_src_files_to_merge = []\n",
    "\n",
    "for tif in urban_tile_files:\n",
    "    src = rasterio.open(tif)\n",
    "    urban_src_files_to_merge.append(src)\n",
    "\n",
    "# Perform the merge for the urban mosaic\n",
    "urban_mosaic, urban_out_trans = merge(urban_src_files_to_merge)\n",
    "\n",
    "# Select bands B02 (Blue), B03 (Green), B04 (Red) for RGB\n",
    "rgb_stack = get_mosaics_by_indices([0], tiles_gdf, ['B02', 'B03', 'B04', 'B08'],\n",
    "                                   \"2016-01-01\", \"2016-02-01\")\n",
    "\n",
    "# Select the bands B02 (Blue), B03 (Green), B04 (Red) for RGB\n",
    "rgb_stack = rgb_stack.sel(band=[\"B02\", \"B03\", \"B04\"])\n",
    "\n",
    "rgb_array = rgb_stack.isel(time=0)  # Select the first (and only) time step\n",
    "\n",
    "# Rearrange dimensions for plotting as RGB\n",
    "rgb_array = rgb_array.transpose('y', 'x', 'band') / rgb_array.max()\n",
    "\n",
    "# Plot the urban and satellite mosaics side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plot the urban mosaic\n",
    "rasterio.plot.show(urban_mosaic, transform=urban_out_trans, ax=ax1, cmap='viridis')\n",
    "ax1.set_title(\"Urban Mosaic\")\n",
    "\n",
    "# Plot the satellite RGB mosaic\n",
    "ax2.imshow(rgb_array)\n",
    "ax2.set_title(f\"Satellite Mosaic (RGB - {specific_date})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subdivided_tiles(input_tile_dir, output_merged_file):\n",
    "    \"\"\"\n",
    "    Merge all subdivided tiles in the input directory into a single raster file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_tile_dir (str): The directory containing the subdivided tiles (.tif files).\n",
    "    - output_merged_file (str): The path to save the merged raster file.\n",
    "    \"\"\"\n",
    "    # Collect all the .tif files in the input directory\n",
    "    tile_files = glob.glob(os.path.join(input_tile_dir, \"*.tif\"))\n",
    "    \n",
    "    if not tile_files:\n",
    "        print(f\"No .tif files found in {input_tile_dir}\")\n",
    "        return\n",
    "    \n",
    "    # List to hold the opened rasterio datasets\n",
    "    src_files_to_merge = []\n",
    "\n",
    "    for tif in tile_files:\n",
    "        src = rasterio.open(tif)\n",
    "        src_files_to_merge.append(src)\n",
    "\n",
    "    # Perform the merge\n",
    "    mosaic, out_trans = merge(src_files_to_merge)\n",
    "\n",
    "    # Get metadata from one of the source files\n",
    "    out_meta = src_files_to_merge[0].meta.copy()\n",
    "\n",
    "    # Update the metadata to reflect the new dimensions, transform, and CRS\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans,\n",
    "        \"count\": mosaic.shape[0],  # Update count to match the number of bands in the mosaic\n",
    "        \"dtype\": mosaic.dtype  # Make sure the data type is set properly\n",
    "    })\n",
    "\n",
    "    # Write the merged raster to disk, ensuring all bands are written correctly\n",
    "    with rasterio.open(output_merged_file, \"w\", **out_meta) as dest:\n",
    "        for i in range(1, mosaic.shape[0] + 1):\n",
    "            dest.write(mosaic[i - 1], i)\n",
    "\n",
    "    # Close the opened files\n",
    "    for src in src_files_to_merge:\n",
    "        src.close()\n",
    "\n",
    "    print(f\"Merged raster saved to {output_merged_file}\")\n",
    "\n",
    "# Define your parameters\n",
    "input_tile_dir = \"../data/subdivided_urban_tiles_gdb\"\n",
    "output_merged_file = \"../data/merged_urban_raster.tif\"\n",
    "\n",
    "if not os.path.exists(output_merged_file):\n",
    "    # Call the function to merge the subdivided tiles\n",
    "    merge_subdivided_tiles(input_tile_dir, output_merged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(output_merged_file) as urban_resampled:\n",
    "    rasterio.plot.show(urban_resampled, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_centroid_lat_lon(stack):\n",
    "    # Extract the x and y coordinates\n",
    "    x_coords = stack['x'].values\n",
    "    y_coords = stack['y'].values\n",
    "    # Compute the centroid in the dataset's CRS\n",
    "    x_min = x_coords.min()\n",
    "    x_max = x_coords.max()\n",
    "    y_min = y_coords.min()\n",
    "    y_max = y_coords.max()\n",
    "    x_centroid = (x_min + x_max) / 2\n",
    "    y_centroid = (y_min + y_max) / 2\n",
    "    # Create a GeoDataFrame with the centroid point\n",
    "    centroid_point = Point(x_centroid, y_centroid)\n",
    "    gdf = gpd.GeoDataFrame(geometry=[centroid_point], crs=stack.rio.crs)\n",
    "    # Reproject to WGS84 (EPSG:4326)\n",
    "    gdf_wgs84 = gdf.to_crs('EPSG:4326')\n",
    "    # Extract latitude and longitude\n",
    "    lon = gdf_wgs84.geometry.x.values[0]\n",
    "    lat = gdf_wgs84.geometry.y.values[0]\n",
    "    return lat, lon\n",
    "\n",
    "# Use the function on your stack\n",
    "lat, lon = get_stack_centroid_lat_lon(stack)\n",
    "print(f'Centroid Latitude: {lat}, Longitude: {lon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `git clone https://github.com/Clay-foundation/model.git`\n",
    "# Download clay-v1-base.ckpt from https://huggingface.co/made-with-clay/Clay/tree/main\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "ckpt = \"../clay-ckpt/clay-v1-base.ckpt\"\n",
    "metadata_path = \"../model/configs/metadata.yaml\" \n",
    "torch.set_default_device(device)\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=metadata_path, shuffle=False, mask_ratio=0\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean, std, and wavelengths from metadata\n",
    "platform = \"sentinel-2-l2a\"\n",
    "metadata = Box(yaml.safe_load(open(metadata_path)))\n",
    "mean = []\n",
    "std = []\n",
    "waves = []\n",
    "# Use the band names to get the correct values in the correct order.\n",
    "for band in stack.band.values:\n",
    "    mean.append(metadata[platform].bands.mean[bands_map[str(band)]])\n",
    "    std.append(metadata[platform].bands.std[bands_map[str(band)]])\n",
    "    waves.append(metadata[platform].bands.wavelength[bands_map[str(band)]])\n",
    "\n",
    "# Prepare the normalization transform function using the mean and std values.\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep datetimes embedding using a normalization function from the model code.\n",
    "def normalize_timestamp(date):\n",
    "    week = date.isocalendar().week * 2 * np.pi / 52\n",
    "    hour = date.hour * 2 * np.pi / 24\n",
    "\n",
    "    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "\n",
    "datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "week_norm = [dat[0] for dat in times]\n",
    "hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "# Prep lat/lon embedding using the\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "lat_norm = [dat[0] for dat in latlons]\n",
    "lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "# Normalize pixels\n",
    "pixels = torch.from_numpy(stack.data.astype(np.float32))\n",
    "pixels = transform(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare additional information\n",
    "datacube = {\n",
    "    \"platform\": platform,\n",
    "    \"time\": torch.tensor(\n",
    "        np.hstack((week_norm, hour_norm)),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    ),\n",
    "    \"latlon\": torch.tensor(\n",
    "        np.hstack((lat_norm, lon_norm)), \n",
    "        dtype=torch.float32, device=device\n",
    "    ),\n",
    "    \"pixels\": pixels.to(device),\n",
    "    \"gsd\": torch.tensor(0.6, device=device),\n",
    "    \"waves\": torch.tensor(waves, device=device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    unmsk_patch, unmsk_idx, msk_idx, msk_matrix = model.model.encoder(datacube)\n",
    "\n",
    "# The first embedding is the class token, which is the\n",
    "# overall single embedding. We extract that for PCA below.\n",
    "embeddings = unmsk_patch[:, 0, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.scatter(stack.time, pca_result, color=\"green\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
