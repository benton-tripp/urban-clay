{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../model\")\n",
    "sys.path.append(\"..\")\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "from rasterio import RasterioIOError\n",
    "import xarray as xr\n",
    "import pickle as pkl\n",
    "from datetime import date\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import Point\n",
    "import torch\n",
    "import yaml\n",
    "from box import Box\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from torchvision.transforms import v2\n",
    "from src.model import ClayMAEModule\n",
    "\n",
    "\n",
    "updated_tiles_gdf = gpd.read_file(\"../data/tiles_with_dates.geojson\")\n",
    "county_boundary = gpd.read_file(\"../data/county_boundary.shp\")\n",
    "tiles_data_dir = \"../data/tiles_gdb\"\n",
    "\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\"]\n",
    "bands_map = {\"B02\": \"blue\", \"B03\": \"green\", \"B04\": \"red\", \"B08\": \"nir\"}\n",
    "gsd = 10\n",
    "epsg = 32617\n",
    "\n",
    "# Date Range\n",
    "start = \"2016-01-01\"\n",
    "end = \"2024-08-31\"\n",
    "\n",
    "def select_dates_best_spread(dates_list, num_per_year=4):\n",
    "    # Sort the dates list\n",
    "    dates_list.sort()\n",
    "    \n",
    "    # Determine target months based on the desired number per year\n",
    "    target_months = {\n",
    "        4: [3, 6, 9, 12],  # Default quarters: March, June, September, December\n",
    "        3: [4, 8, 12],     # For 3 dates per year: April, August, December\n",
    "        2: [6, 12],        # For 2 dates per year: June, December\n",
    "        1: [6]             # For 1 date per year: June\n",
    "    }.get(num_per_year, [6])  # Default to June if an unexpected `num_per_year` is given\n",
    "    \n",
    "\n",
    "    # Group dates by year\n",
    "    dates_by_year = defaultdict(list)\n",
    "    for d in dates_list:\n",
    "        dates_by_year[d.year].append(d)\n",
    "    \n",
    "    selected_dates = []\n",
    "    \n",
    "    # Iterate over each year and select the best spread dates\n",
    "    for year in sorted(dates_by_year.keys()):\n",
    "        available_dates = dates_by_year[year]\n",
    "        \n",
    "        for month in target_months:\n",
    "            # Set the target date as the 1st of the target month\n",
    "            target_date_in_year = date(year, month, 1)\n",
    "            \n",
    "            # Find the closest date in the available dates to the target date\n",
    "            closest_date = min(available_dates, key=lambda d: abs(d - target_date_in_year))\n",
    "            \n",
    "            # Check if the selected date is not already in the list\n",
    "            if closest_date not in selected_dates:\n",
    "                selected_dates.append(closest_date)\n",
    "    \n",
    "    # Post-process to ensure cross-year spread is handled\n",
    "    selected_dates.sort()\n",
    "    return selected_dates\n",
    "\n",
    "with open(\"../data/available_dates.pkl\", \"rb\") as f:\n",
    "    available_dates = pkl.load(f)\n",
    "selected_dates = select_dates_best_spread([d for d in available_dates if d !=  date(2017, 5, 16) \n",
    "                                           and d != date(2024, 8, 27)], 3) # 5/16 is basically unavailable, and 8/25 is already there\n",
    "\n",
    "# Model\n",
    "# `git clone https://github.com/Clay-foundation/model.git`\n",
    "# Download clay-v1-base.ckpt from https://huggingface.co/made-with-clay/Clay/tree/main\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "ckpt = \"../clay-ckpt/clay-v1-base.ckpt\"\n",
    "metadata_path = \"../model/configs/metadata.yaml\" \n",
    "torch.set_default_device(device)\n",
    "\n",
    "model = ClayMAEModule.load_from_checkpoint(\n",
    "    ckpt, metadata_path=metadata_path, shuffle=False, mask_ratio=0\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "def get_by_idx(tile_idx, gdf, bands):\n",
    "    tile_data_files = json.loads(gdf.at[tile_idx, 'date_files_mapping'])\n",
    "    datasets = []\n",
    "    for query_date in list(tile_data_files.keys()):\n",
    "        data_file = tile_data_files[query_date][0]\n",
    "        ds = rioxarray.open_rasterio(data_file)\n",
    "        date_str = ds.time\n",
    "        date = np.datetime64(date_str, 'ns')\n",
    "        ds = ds.assign_coords(band=bands)\n",
    "        ds = ds.expand_dims({'time': [date]})\n",
    "        datasets.append(ds)\n",
    "    stacked_ds = xr.concat(datasets, dim='time')\n",
    "    return stacked_ds\n",
    "\n",
    "def get_stack_centroid_lat_lon(stack):\n",
    "    # Extract the x and y coordinates\n",
    "    x_coords = stack['x'].values\n",
    "    y_coords = stack['y'].values\n",
    "    # Compute the centroid in the dataset's CRS\n",
    "    x_min = x_coords.min()\n",
    "    x_max = x_coords.max()\n",
    "    y_min = y_coords.min()\n",
    "    y_max = y_coords.max()\n",
    "    x_centroid = (x_min + x_max) / 2\n",
    "    y_centroid = (y_min + y_max) / 2\n",
    "    # Create a GeoDataFrame with the centroid point\n",
    "    centroid_point = Point(x_centroid, y_centroid)\n",
    "    gdf = gpd.GeoDataFrame(geometry=[centroid_point], crs=stack.rio.crs)\n",
    "    # Reproject to WGS84 (EPSG:4326)\n",
    "    gdf_wgs84 = gdf.to_crs('EPSG:4326')\n",
    "    # Extract latitude and longitude\n",
    "    lon = gdf_wgs84.geometry.x.values[0]\n",
    "    lat = gdf_wgs84.geometry.y.values[0]\n",
    "    return lat, lon\n",
    "\n",
    "def stack_transform(stack, bands_map, platform=\"sentinel-2-l2a\"):\n",
    "    metadata = Box(yaml.safe_load(open(metadata_path)))\n",
    "    # Extract mean, std, and wavelengths from metadata\n",
    "    mean = []\n",
    "    std = []\n",
    "    waves = []\n",
    "    # Use the band names to get the correct values in the correct order.\n",
    "    for band in stack.band.values:\n",
    "        mean.append(metadata[platform].bands.mean[bands_map[str(band)]])\n",
    "        std.append(metadata[platform].bands.std[bands_map[str(band)]])\n",
    "        waves.append(metadata[platform].bands.wavelength[bands_map[str(band)]])\n",
    "\n",
    "    # Prepare the normalization transform function using the mean and std values.\n",
    "    transform = v2.Compose(\n",
    "        [\n",
    "            v2.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    return mean, std, waves, transform \n",
    "\n",
    "# Prep datetimes embedding using a normalization function from the model code.\n",
    "def normalize_timestamp(date):\n",
    "    week = date.isocalendar().week * 2 * np.pi / 52\n",
    "    hour = date.hour * 2 * np.pi / 24\n",
    "\n",
    "    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))\n",
    "\n",
    "# Prep lat/lon embedding using the\n",
    "def normalize_latlon(lat, lon):\n",
    "    lat = lat * np.pi / 180\n",
    "    lon = lon * np.pi / 180\n",
    "\n",
    "    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))\n",
    "\n",
    "def get_datacube(stack, device, bands_map, platform=\"sentinel-2-l2a\"):\n",
    "\n",
    "    _mean, _std, waves, transform = stack_transform(stack, bands_map, platform)\n",
    "\n",
    "    datetimes = stack.time.values.astype(\"datetime64[s]\").tolist()\n",
    "    times = [normalize_timestamp(dat) for dat in datetimes]\n",
    "    week_norm = [dat[0] for dat in times]\n",
    "    hour_norm = [dat[1] for dat in times]\n",
    "\n",
    "    lat, lon = get_stack_centroid_lat_lon(stack)\n",
    "\n",
    "    latlons = [normalize_latlon(lat, lon)] * len(times)\n",
    "    lat_norm = [dat[0] for dat in latlons]\n",
    "    lon_norm = [dat[1] for dat in latlons]\n",
    "\n",
    "    # Normalize pixels\n",
    "    pixels = torch.from_numpy(stack.data.astype(np.float32))\n",
    "    pixels = transform(pixels)\n",
    "\n",
    "    # Prepare additional information\n",
    "    datacube = {\n",
    "        \"platform\": platform,\n",
    "        \"time\": torch.tensor(\n",
    "            np.hstack((week_norm, hour_norm)),\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "        \"latlon\": torch.tensor(\n",
    "            np.hstack((lat_norm, lon_norm)), \n",
    "            dtype=torch.float32, device=device\n",
    "        ),\n",
    "        \"pixels\": pixels.to(device),\n",
    "        \"gsd\": torch.tensor(0.6, device=device),\n",
    "        \"waves\": torch.tensor(waves, device=device),\n",
    "    }\n",
    "    return datacube\n",
    "\n",
    "def get_embeddings(datacube):\n",
    "    with torch.no_grad():\n",
    "        unmsk_patch, unmsk_idx, msk_idx, msk_matrix = model.model.encoder(datacube)\n",
    "\n",
    "    # The first embedding is the class token, which is the overall single embedding. \n",
    "    embeddings = unmsk_patch[:, 0, :].cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = get_by_idx(0, updated_tiles_gdf, bands)\n",
    "\n",
    "datacube = get_datacube(stack, device, bands_map)\n",
    "embeddings = get_embeddings(datacube)\n",
    "\n",
    "# Run PCA\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "stack.sel(band=[\"B02\", \"B03\", \"B04\"]).plot.imshow(\n",
    "    row=\"time\", rgb=\"band\", vmin=0, vmax=2000, col_wrap=5\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks(rotation=-45)\n",
    "plt.scatter(stack.time, pca_result, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches shape: torch.Size([25, 49, 768])\n",
      "Grid size: 7\n",
      "Stored embeddings for index 0 in the GDF\n"
     ]
    }
   ],
   "source": [
    "updated_tiles_gdf[\"embeddings\"] = None\n",
    "\n",
    "for idx in range(len(updated_tiles_gdf)):\n",
    "    try:\n",
    "        stack = get_by_idx(idx, updated_tiles_gdf, bands)\n",
    "        datacube = get_datacube(stack, device, bands_map)\n",
    "        embeddings = get_embeddings(datacube)\n",
    "\n",
    "        updated_tiles_gdf.at[idx, \"embeddings\"] = embeddings\n",
    "        print(f\"Stored embeddings for index {idx} in the GDF\")\n",
    "    except RasterioIOError as e:\n",
    "        print(f\"Error at index {idx}:\")\n",
    "        print(e)\n",
    "\n",
    "with open(\"../data/gdf_with_embeddings.pkl\", \"wb\") as f:\n",
    "    pkl.dump(updated_tiles_gdf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
