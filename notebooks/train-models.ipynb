{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(19)\n",
    "\n",
    "df = pd.read_csv(\"../data/data_final.csv.gz\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "to_pred_df = df.loc[df['date'] >= pd.to_datetime('2017-01-01 00:00:00')].reset_index(drop=True)\n",
    "df = df.loc[df['date'] < pd.to_datetime('2017-01-01 00:00:00')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = df['tile_index'].unique().tolist()\n",
    "random.shuffle(tiles)\n",
    "\n",
    "train_ratio = 0.75\n",
    "split_index = int(train_ratio * len(tiles))\n",
    "train_tiles = tiles[:split_index] \n",
    "test_tiles = tiles[split_index:]  \n",
    "\n",
    "train_df = df.loc[df['tile_index'].isin(train_tiles)]\n",
    "test_df = df.loc[df['tile_index'].isin(test_tiles)]\n",
    "\n",
    "X_cols = [col for col in df.columns if col.startswith('feature')]\n",
    "y_col = 'avg_urban_imperviousness'\n",
    "X_train = train_df[X_cols]\n",
    "y_train = train_df[y_col]\n",
    "X_test = test_df[X_cols]\n",
    "y_test = test_df[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13395, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       avg_urban_imperviousness\n",
      "count              13395.000000\n",
      "mean                   3.210876\n",
      "std                    5.788118\n",
      "min                    0.000000\n",
      "25%                    0.366940\n",
      "50%                    1.387734\n",
      "75%                    3.229994\n",
      "max                   59.641462\n"
     ]
    }
   ],
   "source": [
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 5.7381\n",
      "Baseline R^2: -0.0002\n"
     ]
    }
   ],
   "source": [
    "# Establish a baseline model using the mean\n",
    "baseline_pred = np.full_like(y_test, y_train.mean().values, dtype=np.float64)\n",
    "\n",
    "# Calculate baseline performance\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "baseline_r2 = r2_score(y_test, baseline_pred)\n",
    "print(f\"Baseline RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"Baseline R^2: {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 2.5847\n",
      "Linear Regression R^2: 0.7971\n"
     ]
    }
   ],
   "source": [
    "# Simple Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate Linear Regression model\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")\n",
    "print(f\"Linear Regression R^2: {lr_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA + Linear Regression RMSE: 2.6655\n",
      "PCA + Linear Regression R^2: 0.7842\n"
     ]
    }
   ],
   "source": [
    "# PCA + Linear Regression\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr_pca_model = LinearRegression()\n",
    "lr_pca_model.fit(X_train_pca, y_train)\n",
    "lr_pca_pred = lr_pca_model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate PCA + Linear Regression model\n",
    "lr_pca_rmse = np.sqrt(mean_squared_error(y_test, lr_pca_pred))\n",
    "lr_pca_r2 = r2_score(y_test, lr_pca_pred)\n",
    "print(f\"PCA + Linear Regression RMSE: {lr_pca_rmse:.4f}\")\n",
    "print(f\"PCA + Linear Regression R^2: {lr_pca_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\btripp\\urban-clay\\env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO Regression RMSE: 2.5538\n",
      "LASSO Regression R^2: 0.8019\n",
      "Optimal alpha for LASSO: 5.4287e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\btripp\\urban-clay\\env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.452e+02, tolerance: 4.487e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# LASSO Regression with Cross-Validation\n",
    "lasso_model = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "lasso_pred = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate LASSO model\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\n",
    "lasso_r2 = r2_score(y_test, lasso_pred)\n",
    "print(f\"LASSO Regression RMSE: {lasso_rmse:.4f}\")\n",
    "print(f\"LASSO Regression R^2: {lasso_r2:.4f}\")\n",
    "print(f\"Optimal alpha for LASSO: {lasso_model.alpha_:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with Cross-Validation for Hyperparameter Tuning\n",
    "rf_model = RandomForestRegressor(random_state=19)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf_cv = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_cv.fit(X_train, y_train.values.ravel())\n",
    "rf_best = rf_cv.best_estimator_\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"Random Forest R^2: {rf_r2:.4f}\")\n",
    "print(f\"Best params for Random Forest: {rf_cv.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.851744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.093454\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.162310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.378563\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.568309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.851744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.093454\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.162310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.378563\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.568309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.851744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.093454\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.162310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.378563\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.568309\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.851744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 10716, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 3.093454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LightGBM with Cross-Validation for Hyperparameter Tuning\n",
    "lgb_model = LGBMRegressor(random_state=19)\n",
    "lgb_param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "lgb_cv = GridSearchCV(lgb_model, lgb_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "lgb_cv.fit(X_train, y_train.values.ravel())\n",
    "lgb_best = lgb_cv.best_estimator_\n",
    "lgb_pred = lgb_best.predict(X_test)\n",
    "\n",
    "# Evaluate LightGBM model\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))\n",
    "lgb_r2 = r2_score(y_test, lgb_pred)\n",
    "print(f\"LightGBM RMSE: {lgb_rmse:.4f}\")\n",
    "print(f\"LightGBM R^2: {lgb_r2:.4f}\")\n",
    "print(f\"Best params for LightGBM: {lgb_cv.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the model results\n",
    "results = {\n",
    "    'Model': ['Baseline', 'Linear Regression', 'PCA + Linear Regression', 'LASSO Regression', 'Random Forest', 'LightGBM'],\n",
    "    'RMSE': [baseline_rmse, lr_rmse, lr_pca_rmse, lasso_rmse, rf_rmse, lgb_rmse],\n",
    "    'R^2': [baseline_r2, lr_r2, lr_pca_r2, lasso_r2, rf_r2, lgb_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Plot the scores for easy comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# RMSE Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(results_df['Model'], results_df['RMSE'], color='skyblue')\n",
    "plt.xlabel('RMSE')\n",
    "plt.title('RMSE of Models')\n",
    "\n",
    "# R^2 Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(results_df['Model'], results_df['R^2'], color='lightgreen')\n",
    "plt.xlabel('R^2 Score')\n",
    "plt.title('R^2 Score of Models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
