{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping field data_files: unsupported OGR type: 5\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "county_boundary = gpd.read_file(\"../data/county_boundary.shp\")\n",
    "tiles_data_dir = \"../data/tiles_gdb\"\n",
    "tiles_geojson_path = \"../data/tiles.geojson\"\n",
    "tiles_gdf = gpd.read_file(tiles_geojson_path)\n",
    "\n",
    "with open('../data/failed_downloads.pkl', 'rb') as f:\n",
    "    failed_downloads = pkl.load(f)\n",
    "\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1/\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "query = {\"eo:cloud_cover\": {\"lt\": 1}}\n",
    "\n",
    "collections=[\"sentinel-2-l2a\"]\n",
    "\n",
    "# Dates to query\n",
    "start = \"2016-01-01\"\n",
    "end = \"2024-08-31\"\n",
    "\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\"]\n",
    "bands_map = {\"B02\": \"blue\", \"B03\": \"green\", \"B04\": \"red\", \"B08\": \"nir\"}\n",
    "gsd = 10\n",
    "epsg = 32617\n",
    "\n",
    "\n",
    "def select_dates_best_spread(dates_list, num_per_year=4):\n",
    "    # Sort the dates list\n",
    "    dates_list.sort()\n",
    "\n",
    "    # Define target months based on the desired number per year\n",
    "    target_months = {\n",
    "        4: [3, 6, 9, 12],  # Default quarters: March, June, September, December\n",
    "        3: [4, 8, 12],     # For 3 dates per year: April, August, December\n",
    "        2: [6, 12],        # For 2 dates per year: June, December\n",
    "        1: [6]             # For 1 date per year: June\n",
    "    }.get(num_per_year, [6])  # Default to June if an unexpected `num_per_year` is given\n",
    "\n",
    "    # Group dates by year\n",
    "    dates_by_year = defaultdict(list)\n",
    "    for d in dates_list:\n",
    "        dates_by_year[d.year].append(d)\n",
    "    \n",
    "    selected_dates = []\n",
    "\n",
    "    # Iterate over each year and select dates with the best spread\n",
    "    for year, available_dates in dates_by_year.items():\n",
    "        yearly_selected = set()\n",
    "        \n",
    "        for month in target_months:\n",
    "            target_date = date(year, month, 1)\n",
    "            \n",
    "            # Find the closest date to the target date in available_dates, if any\n",
    "            closest_date = min(\n",
    "                (d for d in available_dates if d not in yearly_selected),\n",
    "                key=lambda d: abs(d - target_date),\n",
    "                default=None\n",
    "            )\n",
    "            \n",
    "            # Append the closest date if available\n",
    "            if closest_date:\n",
    "                yearly_selected.add(closest_date)\n",
    "        \n",
    "        # Extend selected dates with unique entries for the year\n",
    "        selected_dates.extend(sorted(yearly_selected))\n",
    "    \n",
    "    # Ensure final selection is sorted\n",
    "    selected_dates.sort()\n",
    "    return selected_dates\n",
    "\n",
    "with open(\"../data/available_dates.pkl\", \"rb\") as f:\n",
    "    available_dates = pkl.load(f)\n",
    "\n",
    "buffer_days = 45\n",
    "not_available_dates = [date(2022, 4, 3), date(2022, 2, 14), date(2021, 9, 27),  date(2021, 8, 8),\n",
    "                       date(2020, 12, 1), date(2020, 10, 7), date(2019, 4, 4), date(2019, 2, 5), \n",
    "                       date(2018, 11, 27), date(2018, 10, 1)]\n",
    "omit_dates = [date(2024, 6, 11), date(2022, 6, 17), date(2021, 4, 30), date(2020, 7, 12), \n",
    "              date(2019, 7, 30), date(2018, 5, 11), date(2017, 9, 8)]\n",
    "selected_dates = select_dates_best_spread([d for d in available_dates if d !=  date(2017, 5, 16) and d != date(2024, 8, 27)\n",
    "                                           and all(not (nd - timedelta(days=buffer_days) <= d <= nd + timedelta(days=buffer_days)) \n",
    "                                                   for nd in not_available_dates)], 3) \n",
    "selected_dates = [d for d in selected_dates if d not in omit_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query STAC items for a tile\n",
    "def query_stac_tile(tile_geometry, catalog, start, end, \n",
    "                    query, collections=[\"sentinel-2-l2a\"], \n",
    "                    limit=1000):\n",
    "    # Get the bounds of the tile in WGS84\n",
    "    tile_wgs84 = gpd.GeoSeries([tile_geometry], crs=\"EPSG:32617\").to_crs(\"EPSG:4326\").iloc[0]\n",
    "    minx, miny, maxx, maxy = tile_wgs84.bounds\n",
    "    bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Perform the search\n",
    "    search = catalog.search(\n",
    "        collections=collections,\n",
    "        bbox=bbox,\n",
    "        datetime=f\"{start}/{end}\",\n",
    "        limit=limit,\n",
    "        query=query\n",
    "    )\n",
    "    # Get the items from the search results\n",
    "    items = list(search.item_collection())\n",
    "    return items\n",
    "\n",
    "def get_subregion(dataset, bounds, epsilon=0.):\n",
    "    min_x, min_y, max_x, max_y = bounds\n",
    "    # Subset the dataset using xarray's sel function\n",
    "    subregion = dataset.sel(\n",
    "        x=slice(min_x - epsilon, max_x + epsilon),  # X-coordinate bounds\n",
    "        y=slice(max_y + epsilon, min_y - epsilon)   # Y-coordinate bounds (flip due to coordinate system)\n",
    "    )\n",
    "    return subregion\n",
    "\n",
    "# Function to get images from item\n",
    "def get_images_from_item(item, bands, output_file_path, \n",
    "                         chunk_size=2048, dtype=np.float32, \n",
    "                         crs=\"EPSG:32617\", bounds=None, epsilon=0.):\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "\n",
    "        band_datasets = []\n",
    "        # Loop through bands and collect data\n",
    "        for band in bands:\n",
    "            # Sign the asset URLs\n",
    "            asset_href = planetary_computer.sign(item.assets[band].href)\n",
    "            # Open the image using rioxarray\n",
    "            with rasterio.Env():\n",
    "                ds = rioxarray.open_rasterio(\n",
    "                    asset_href,\n",
    "                    chunks={\"band\": -1, \"x\": chunk_size, \"y\": chunk_size},\n",
    "                    lock=False\n",
    "                ).astype(dtype)\n",
    "                if ds.rio.crs != crs:\n",
    "                    ds = ds.rio.reproject(\n",
    "                        crs, \n",
    "                        resampling=Resampling.nearest,\n",
    "                        num_threads=2\n",
    "                    )\n",
    "                if bounds is not None:\n",
    "                    ds = get_subregion(ds, bounds, epsilon=epsilon)\n",
    "                band_datasets.append(ds)\n",
    "\n",
    "        # Stack bands into a single dataset\n",
    "        stacked_ds = xr.concat(band_datasets, dim='band')\n",
    "\n",
    "        # Store time as an attribute\n",
    "        naive_datetime = item.datetime.replace(tzinfo=None)\n",
    "        time_value = np.datetime64(naive_datetime, 'ns')\n",
    "        stacked_ds.attrs['time'] = str(time_value)\n",
    "        \n",
    "        # Save the stacked dataset to a single GeoTIFF file\n",
    "        # stacked_ds.rio.to_raster(output_file_path)\n",
    "        # Write the data using Dask and rioxarray, with windowed=True and tiled=True\n",
    "        with rasterio.Env(GDAL_CACHEMAX=512):  # Set cache size to 512 MB\n",
    "            stacked_ds.rio.to_raster(\n",
    "                output_file_path,\n",
    "                tiled=True,\n",
    "                windowed=True,\n",
    "                blockxsize=256,\n",
    "                blockysize=256,\n",
    "                compress=\"deflate\",\n",
    "                num_threads=2,\n",
    "                bigtiff='yes'\n",
    "            )\n",
    "    return output_file_path\n",
    "\n",
    "# Function to clean up bounds for filename\n",
    "def clean_bounds(bounds):\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    # Round to 3 decimal places and remove any special characters\n",
    "    minx_str = f\"{minx:.3f}\".replace('.', '_')\n",
    "    miny_str = f\"{miny:.3f}\".replace('.', '_')\n",
    "    maxx_str = f\"{maxx:.3f}\".replace('.', '_')\n",
    "    maxy_str = f\"{maxy:.3f}\".replace('.', '_')\n",
    "    # Combine into a single string\n",
    "    bounds_str = f\"{minx_str}_{miny_str}_{maxx_str}_{maxy_str}\"\n",
    "    return bounds_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-querying tile for date: 2022-05-28 at coordinates: (750600.0, 3913800.0, 751200.0, 3914400.0)\n",
      "Saved re-queried tile to ../data/tiles_gdb\\tile_4726_750600_000_3913800_000_751200_000_3914400_000_2022-05-28_B02_B03_B04_B08.tif\n"
     ]
    }
   ],
   "source": [
    "from rioxarray.exceptions import NoDataInBounds\n",
    "\n",
    "def requery_failed_downloads(failed_downloads, catalog, query, \n",
    "                          collections=[\"sentinel-2-l2a\"], limit=1000, \n",
    "                          save_dir=\"../data/tiles_gdb\", buffer_days=60):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    no_data_in_bounds = []\n",
    "    for failed_download in failed_downloads:\n",
    "        idx = failed_download['idx'] \n",
    "        tile_polygon = failed_download['tile_geometry']  \n",
    "        query_date = failed_download['query_date']\n",
    "        buffer_days = 0  # Start without buffer\n",
    "        print(f\"Re-querying tile for date: {query_date} at coordinates: {failed_download['tile_bounds']}\")\n",
    "        \n",
    "        while buffer_days <= buffer_days: \n",
    "            start_date = query_date - timedelta(days=buffer_days)\n",
    "            end_date = query_date + timedelta(days=buffer_days)\n",
    "            \n",
    "            # Query the STAC catalog for the missing tile\n",
    "            items = query_stac_tile(tile_polygon, catalog, start_date.isoformat(), end_date.isoformat(), \n",
    "                                    query, collections=collections, limit=limit)\n",
    "            \n",
    "            if items:\n",
    "                for item in items:\n",
    "                    saved = False\n",
    "                    # Define the output file path before querying\n",
    "                    tile_bounds = tile_polygon.bounds\n",
    "                    bounds_str = clean_bounds(tile_bounds)\n",
    "                    band_sfx = \"_\".join(bands)\n",
    "                    output_file = os.path.join(tiles_data_dir, f\"tile_{idx}_{bounds_str}_{query_date.isoformat()}_{band_sfx}.tif\")\n",
    "                    try:\n",
    "                        if not os.path.exists(output_file):\n",
    "                            # Save the queried image\n",
    "                            get_images_from_item(item, bands, output_file, bounds=tile_polygon.bounds, epsilon=0.001)\n",
    "                            print(f\"Saved re-queried tile to {output_file}\")\n",
    "                        saved = True\n",
    "                    except NoDataInBounds as e:\n",
    "                        print(e)\n",
    "                if not saved:\n",
    "                    no_data_in_bounds.append({\n",
    "                            'idx': idx,\n",
    "                            'tile_polygon': tile_polygon,\n",
    "                            'query_date': query_date \n",
    "                        })\n",
    "                    print(f\"Appended {tile_polygon.bounds} {query_date} to `no_data_in_bounds`.\")\n",
    "                \n",
    "                break  # Stop if data is found and saved\n",
    "            \n",
    "            buffer_days += 2  # Increase the buffer by 2 days if no data found\n",
    "\n",
    "        if buffer_days > buffer_days:\n",
    "            print(f\"Failed to retrieve data for tile {failed_download['tile_bounds']} on date {query_date} within buffer range.\")\n",
    "    # return no_data_in_bounds\n",
    "    return item, bands, output_file, tile_polygon\n",
    "\n",
    "\n",
    "idx = 4726\n",
    "tile_geometry = tiles_gdf.at[idx, \"geometry\"]\n",
    "\n",
    "# no_data_in_bounds = requery_failed_downloads([{\n",
    "item, bands, output_file, tile_polygon = requery_failed_downloads([{\n",
    "    \"idx\": idx,\n",
    "    \"query_date\": date(2022, 5, 28),\n",
    "    \"tile_geometry\": tile_geometry,\n",
    "    \"tile_bounds\": tile_geometry.bounds,\n",
    "    \"bounds_str\": clean_bounds(tile_geometry.bounds),\n",
    "    \"band_sfx\": \"_\".join(bands),\n",
    "    \"bands\":bands\n",
    "}], catalog, query=query, save_dir=\"../data/tiles_gdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned tile bounds: (750600.0, 3913800.0, 751200.0, 3914400.0)\n",
      "Tile bounds: (750600.0, 3913800.0, 751200.0, 3914400.0)\n",
      "Raster bounds: BoundingBox(left=750600.0, bottom=3913800.0, right=751200.0, top=3914400.0)\n",
      "Shape: 60 x 60\n"
     ]
    }
   ],
   "source": [
    "def align_to_pixel_grid(bounds, pixel_size):\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    minx_aligned = np.floor(minx / pixel_size) * pixel_size\n",
    "    miny_aligned = np.floor(miny / pixel_size) * pixel_size\n",
    "    maxx_aligned = np.ceil(maxx / pixel_size) * pixel_size\n",
    "    maxy_aligned = np.ceil(maxy / pixel_size) * pixel_size\n",
    "    return minx_aligned, miny_aligned, maxx_aligned, maxy_aligned\n",
    "\n",
    "aligned_bounds = align_to_pixel_grid(tile_polygon.bounds, pixel_size=10)\n",
    "get_images_from_item(item, bands, output_file, bounds=aligned_bounds, epsilon=0.1)\n",
    "rpath = '../data/tiles_gdb/tile_4726_750600_000_3913800_000_751200_000_3914400_000_2022-05-28_B02_B03_B04_B08.tif'\n",
    "with rasterio.open(rpath) as src:\n",
    "    aligned_bounds = align_to_pixel_grid(tile_polygon.bounds, pixel_size=10)\n",
    "    print(\"Aligned tile bounds:\", aligned_bounds)\n",
    "    print(\"Tile bounds:\", tile_polygon.bounds)\n",
    "    bounds = src.bounds\n",
    "    print(\"Raster bounds:\", bounds)\n",
    "    # Get the width and height in pixels\n",
    "    width = src.width\n",
    "    height = src.height\n",
    "\n",
    "# Print the shape (width x height)\n",
    "print(f'Shape: {width} x {height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse the date from the file name.\n",
    "    The date is assumed to be in ISO format (YYYY-MM-DD) within the file name.\n",
    "    The date is typically the part just before the band names, which we assume is the third-to-last part.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize the file path to handle potential backward slashes and extract the filename\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Split by underscore and extract the date part (which is third-to-last based on the example)\n",
    "        parts = base_name.split('_')\n",
    "        # The date part is the one right before the bands (3rd from last)\n",
    "        date_str = parts[-5]\n",
    "        # Convert the date string to a date object\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "    except (ValueError, IndexError) as e:\n",
    "        # In case of any error in parsing, return None and print error for debugging\n",
    "        print(f\"Error parsing date from filename '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "def assign_files_to_dates(tiles_gdf, max_buffer_days, selected_dates, tiles_data_dir='../data/tiles_gdb'):\n",
    "\n",
    "    # Initialize a new column in tiles_gdf to hold the date-to-files mapping\n",
    "    if 'date_files_mapping' not in tiles_gdf.columns:\n",
    "        tiles_gdf['date_files_mapping'] = [{} for _ in range(len(tiles_gdf))]\n",
    "\n",
    "    # Convert selected_dates to strings for GeoJSON compatibility\n",
    "    selected_dates_str = [d.strftime('%Y-%m-%d') for d in selected_dates]\n",
    "\n",
    "    for idx, tile in tiles_gdf.iterrows():\n",
    "        # Get the list of files associated with the current tile\n",
    "        tile_bounds = tile['geometry'].bounds\n",
    "        bounds_str = clean_bounds(tile_bounds)\n",
    "        band_sfx = \"_\".join(bands)\n",
    "        data_files = [os.path.join(tiles_data_dir, f\"tile_{idx}_{bounds_str}_{query_date.isoformat()}_{band_sfx}.tif\") \\\n",
    "                      for query_date in selected_dates]\n",
    "        \n",
    "        # Initialize a dictionary with each selected date (as string) as a key and an empty list as the value\n",
    "        date_files = {selected_date: [] for selected_date in selected_dates_str}\n",
    "        \n",
    "        for file in data_files:\n",
    "            # Extract the date from the file name\n",
    "            file_date = parse_date_from_filename(file)\n",
    "            if not file_date:\n",
    "                continue  # Skip files without valid dates\n",
    "\n",
    "            # Convert file_date to string for comparison\n",
    "            file_date_str = file_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Check if the file date is within the buffer for any of the selected dates\n",
    "            for target_date in selected_dates:\n",
    "                if abs((file_date - target_date).days) <= max_buffer_days:\n",
    "                    # Add the file to the list of files for the matching target date\n",
    "                    date_files[target_date.strftime('%Y-%m-%d')].append(file)\n",
    "        \n",
    "        # Update the 'date_files_mapping' column with the date-to-files dictionary\n",
    "        tiles_gdf.at[idx, 'date_files_mapping'] = date_files\n",
    "    \n",
    "    # Drop the old 'data_files' column since it's now replaced by 'date_files_mapping'\n",
    "    if 'data_files' in tiles_gdf.columns:\n",
    "        tiles_gdf = tiles_gdf.drop(columns=['data_files'])\n",
    "    \n",
    "    return tiles_gdf\n",
    "\n",
    "# Call the function to update the GeoDataFrame\n",
    "updated_tiles_gdf = assign_files_to_dates(tiles_gdf, max_buffer_days=45, selected_dates=selected_dates, tiles_data_dir='../data/tiles_gdb')\n",
    "\n",
    "# Save the updated GeoDataFrame to GeoJSON\n",
    "updated_tiles_geojson_path = \"../data/tiles_with_dates.geojson\"\n",
    "updated_tiles_gdf.to_file(updated_tiles_geojson_path, driver='GeoJSON')\n",
    "\n",
    "with open(updated_tiles_geojson_path, 'r') as file:\n",
    "    geojson_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize containers for results\n",
    "empty_date_records = []\n",
    "multiple_file_records = []\n",
    "non_empty_counts = defaultdict(int)\n",
    "\n",
    "# Iterate over the features\n",
    "for feature in geojson_data['features']:\n",
    "    date_files_mapping = feature['properties']['date_files_mapping']\n",
    "    \n",
    "    # Check for empty lists and lists with more than one file\n",
    "    for date, file_list in date_files_mapping.items():\n",
    "        if len(file_list) == 0:\n",
    "            empty_date_records.append((feature['geometry']['coordinates'], date))\n",
    "        if len(file_list) > 1:\n",
    "            multiple_file_records.append((feature['geometry']['coordinates'], date, file_list))\n",
    "        \n",
    "        # Count non-empty lists per date\n",
    "        if len(file_list) > 0:\n",
    "            non_empty_counts[date] += 1\n",
    "\n",
    "print(f\"Updated tiles GeoDataFrame saved to {updated_tiles_geojson_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tile coordinates and flatten them to ensure they are tuples of floats\n",
    "missing_tiles_coords = [tuple([tuple(coord) for coord in record[0]]) for record in empty_date_records]\n",
    "missing_tiles_coords = [i[0] for i in missing_tiles_coords]\n",
    "\n",
    "# Convert coordinates to polygons for plotting\n",
    "missing_tiles_gdf = gpd.GeoDataFrame(\n",
    "    geometry=[Polygon(coords) for coords in missing_tiles_coords],  # Ensure valid polygons\n",
    "    crs=updated_tiles_gdf.crs\n",
    ").drop_duplicates()\n",
    "\n",
    "# Visualize the tiles, filling in each tile with ANY missing dates (using the unique coordinates from empty_date_records)\n",
    "ax = county_boundary.plot(color='lightgray', figsize=(7, 7))\n",
    "updated_tiles_gdf.boundary.plot(ax=ax, edgecolor='red', linewidth=0.5)\n",
    "# Fill empty tiles with black\n",
    "missing_tiles_gdf.plot(ax=ax, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
